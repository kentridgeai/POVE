{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30193a44-e086-48dd-9639-36d8a8cbb13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Expressions:   0%|                            | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping length 1 (already completed).\n",
      "Skipping cleaning for skeleton_data/len2_features2.csv (file not found).\n",
      "Skipping length 3 (already completed).\n",
      "Skipping cleaning for skeleton_data/len4_features2.csv (file not found).\n",
      "Skipping length 5 (already completed).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Expressions:  55%|██████████▉         | 6/11 [00:00<00:00, 58.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping cleaning for skeleton_data/len6_features2.csv (file not found).\n",
      "Skipping length 7 (already completed).\n",
      "Skipping cleaning for skeleton_data/len8_features2.csv (file not found).\n",
      "Skipping length 9 (already completed).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Expressions: 100%|███████████████████| 11/11 [00:00<00:00, 13.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping cleaning for skeleton_data/len10_features2.csv (file not found).\n",
      "Skipping length 11 (already completed).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Expressions:   0%|                            | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping length 1 (already completed).\n",
      "Skipping cleaning for skeleton_data/len2_features3.csv (file not found).\n",
      "Skipping length 3 (already completed).\n",
      "Skipping cleaning for skeleton_data/len4_features3.csv (file not found).\n",
      "Skipping length 5 (already completed).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Expressions:  55%|██████████▉         | 6/11 [00:00<00:00, 58.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping cleaning for skeleton_data/len6_features3.csv (file not found).\n",
      "Skipping length 7 (already completed).\n",
      "Skipping cleaning for skeleton_data/len8_features3.csv (file not found).\n",
      "Skipping length 9 (already completed).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Expressions: 100%|███████████████████| 11/11 [00:00<00:00, 14.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping cleaning for skeleton_data/len10_features3.csv (file not found).\n",
      "Skipping length 11 (already completed).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Expressions:   0%|                             | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping length 1 (already completed).\n",
      "Skipping cleaning for skeleton_data/len2_features4.csv (file not found).\n",
      "Skipping length 3 (already completed).\n",
      "Skipping cleaning for skeleton_data/len4_features4.csv (file not found).\n",
      "Skipping length 5 (already completed).\n",
      "Skipping cleaning for skeleton_data/len6_features4.csv (file not found).\n",
      "Skipping length 7 (already completed).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Expressions: 100%|█████████████████████| 9/9 [00:00<00:00, 21.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping cleaning for skeleton_data/len8_features4.csv (file not found).\n",
      "Skipping length 9 (already completed).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Expressions:   0%|                             | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping length 1 (already completed).\n",
      "Skipping cleaning for skeleton_data/len2_features5.csv (file not found).\n",
      "Skipping length 3 (already completed).\n",
      "Skipping cleaning for skeleton_data/len4_features5.csv (file not found).\n",
      "Skipping length 5 (already completed).\n",
      "Skipping cleaning for skeleton_data/len6_features5.csv (file not found).\n",
      "Skipping length 7 (already completed).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Expressions: 100%|█████████████████████| 9/9 [00:00<00:00, 21.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping cleaning for skeleton_data/len8_features5.csv (file not found).\n",
      "Skipping length 9 (already completed).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Expressions:   0%|                             | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping length 1 (already completed).\n",
      "Skipping cleaning for skeleton_data/len2_features6.csv (file not found).\n",
      "Skipping length 3 (already completed).\n",
      "Skipping cleaning for skeleton_data/len4_features6.csv (file not found).\n",
      "Skipping length 5 (already completed).\n",
      "Skipping cleaning for skeleton_data/len6_features6.csv (file not found).\n",
      "Skipping length 7 (already completed).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Expressions: 100%|█████████████████████| 9/9 [00:00<00:00, 21.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping cleaning for skeleton_data/len8_features6.csv (file not found).\n",
      "Skipping length 9 (already completed).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Expressions:   0%|                             | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping length 1 (already completed).\n",
      "Skipping cleaning for skeleton_data/len2_features7.csv (file not found).\n",
      "Skipping length 3 (already completed).\n",
      "Skipping cleaning for skeleton_data/len4_features7.csv (file not found).\n",
      "Skipping length 5 (already completed).\n",
      "Skipping cleaning for skeleton_data/len6_features7.csv (file not found).\n",
      "Skipping length 7 (already completed).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Expressions: 100%|█████████████████████| 9/9 [00:00<00:00, 21.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping cleaning for skeleton_data/len8_features7.csv (file not found).\n",
      "Skipping length 9 (already completed).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import ast\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "import re\n",
    "from gplearn_memetic.simplification import count_expression_length, simplify_expression, SimplificationError, CONSTANT, OPERATORS, FUNCTIONS\n",
    "\n",
    "# Function to generate all valid tree structures\n",
    "def generate_valid_trees(k):\n",
    "    if k == 1:\n",
    "        return [[0]]  # Only one valid tree with a single node\n",
    "\n",
    "    valid_trees = []\n",
    "    possible_trees = itertools.product([0, 1, 2], repeat=k)\n",
    "\n",
    "    for tree in possible_trees:\n",
    "        if tree[0] == 0 or tree[-1] != 0:  # First node cannot be nullary, last must be nullary\n",
    "            continue\n",
    "\n",
    "        stack = 1\n",
    "        valid = True\n",
    "\n",
    "        for node in tree:\n",
    "            if stack < 1:\n",
    "                valid = False\n",
    "                break\n",
    "            if node == 0:\n",
    "                stack -= 1\n",
    "            elif node == 1:\n",
    "                stack += 0  # Unary does not affect stack size\n",
    "            elif node == 2:\n",
    "                stack += 1  # Binary increases stack requirement\n",
    "\n",
    "        if valid and stack == 0:  # Only keep valid trees\n",
    "            valid_trees.append(tree)\n",
    "\n",
    "    return valid_trees\n",
    "\n",
    "def build_expressions_from_tree(tree):\n",
    "    expressions = []\n",
    "\n",
    "    # Choices for nullary nodes (variables and constant)\n",
    "    leaf_choices = VARIABLES + [CONSTANT]\n",
    "\n",
    "    # Iterate over all possible ways to assign nullary nodes\n",
    "    for leaf_assignment in itertools.product(leaf_choices, repeat=tree.count(0)):\n",
    "        for unary_assignment in itertools.product(FUNCTIONS, repeat=tree.count(1)):\n",
    "            for binary_assignment in itertools.product(OPERATORS, repeat=tree.count(2)):\n",
    "\n",
    "                stack = []\n",
    "                leaf_idx, unary_idx, binary_idx = 0, 0, 0  # Index tracking\n",
    "                valid = True\n",
    "\n",
    "                # **Process tree from RIGHT to LEFT**\n",
    "                for node in reversed(tree):\n",
    "                    if node == 0:  # Nullary operator (Variable or Constant)\n",
    "                        stack.append(leaf_assignment[leaf_idx])\n",
    "                        leaf_idx += 1\n",
    "                    elif node == 1:  # Unary operator (sin, cos, etc.)\n",
    "                        if len(stack) < 1:\n",
    "                            valid = False  # Invalid if there's no operand\n",
    "                            break\n",
    "                        arg = stack.pop()\n",
    "                        stack.append(f\"{unary_assignment[unary_idx]}({arg})\")\n",
    "                        unary_idx += 1\n",
    "                    elif node == 2:  # Binary operator (+, -, *, /)\n",
    "                        if len(stack) < 2:\n",
    "                            valid = False  # Invalid if less than 2 operands exist\n",
    "                            break\n",
    "                        right = stack.pop()\n",
    "                        left = stack.pop()\n",
    "                        stack.append(f\"({left} {binary_assignment[binary_idx]} {right})\")\n",
    "                        binary_idx += 1\n",
    "\n",
    "                # **Ensure exactly 1 final expression remains**\n",
    "                if valid and len(stack) == 1:\n",
    "                    expressions.append(stack[0])\n",
    "\n",
    "    return expressions\n",
    "\n",
    "def generate_expressions(max_length, save_interval, num_features):\n",
    "    for length in tqdm(range(1, max_length + 1), desc=\"Generating Expressions\"):\n",
    "        csv_filename = f\"skeleton_data/len{length}_features{num_features}.csv\"\n",
    "        feather_filename = f\"skeleton_data/len{length}_features{num_features}.feather\"\n",
    "\n",
    "        # Skip this length if the feather file exists (indicating the current length is fully completed)\n",
    "        if os.path.exists(feather_filename):\n",
    "            print(f\"Skipping length {length} (already completed).\")\n",
    "            continue\n",
    "\n",
    "        valid_trees = generate_valid_trees(length)  # Generate valid trees\n",
    "\n",
    "        num_workers = min(max(1, multiprocessing.cpu_count() - 1), len(valid_trees))  # Limit workers to available cores\n",
    "        batch = set()\n",
    "\n",
    "        with multiprocessing.Pool(num_workers) as pool:\n",
    "            for expressions in pool.imap_unordered(process_tree, [(tree, length) for tree in valid_trees]):\n",
    "                batch.update(expressions)  # Add unique expressions to the batch\n",
    "\n",
    "                # Save when batch reaches save_interval\n",
    "                if len(batch) >= save_interval:\n",
    "                    append_to_csv(csv_filename, batch)\n",
    "                    batch.clear()  # Clear batch after saving\n",
    "\n",
    "        # Save any remaining expressions\n",
    "        if batch:\n",
    "            append_to_csv(csv_filename, batch)\n",
    "\n",
    "        # **Clean and Sort the CSV File After Processing**\n",
    "        clean_and_sort_csv(csv_filename, feather_filename)\n",
    "\n",
    "def process_tree(args):\n",
    "    tree, length = args\n",
    "    expressions = set()\n",
    "    raw_expressions = build_expressions_from_tree(tree)\n",
    "\n",
    "    for expr in raw_expressions:\n",
    "        simplified_expr = simplify_expression(expr)\n",
    "        if (count_expression_length(simplified_expr) == length) or ('1' in simplified_expr) or ('0' in simplified_expr):\n",
    "            expressions.add(simplified_expr)  # Ensure uniqueness at worker level\n",
    "\n",
    "    return expressions\n",
    "\n",
    "def append_to_csv(filename, expressions):\n",
    "    if not expressions:\n",
    "        return  # Avoid writing empty data\n",
    "\n",
    "    df = pd.DataFrame(list(expressions), columns=['Expression'])\n",
    "    df.to_csv(filename, mode='a', index=False, header=not os.path.exists(filename))\n",
    "\n",
    "def clean_and_sort_csv(csv_filename, feather_filename):\n",
    "    if not os.path.exists(csv_filename):\n",
    "        print(f\"Skipping cleaning for {csv_filename} (file not found).\")\n",
    "        return\n",
    "    \n",
    "    # Load current CSV\n",
    "    df = pd.read_csv(csv_filename)\n",
    "    \n",
    "    # Extract len and feature count from filename\n",
    "    match = re.search(r'len(\\d+)_features(\\d+)', csv_filename)\n",
    "    if not match:\n",
    "        raise ValueError(f\"Filename format not recognized: {csv_filename}\")\n",
    "    \n",
    "    current_len = int(match.group(1))\n",
    "    feature_count = match.group(2)\n",
    "    directory = os.path.dirname(csv_filename)\n",
    "    \n",
    "    # Collect expressions from smaller len files with the same feature count\n",
    "    existing_expressions = set()\n",
    "    \n",
    "    for file in os.listdir(directory):\n",
    "        file_match = re.search(r'len(\\d+)_features' + feature_count + r'\\.csv$', file)\n",
    "        if file_match:\n",
    "            file_len = int(file_match.group(1))\n",
    "            if file_len < current_len:\n",
    "                small_df = pd.read_csv(os.path.join(directory, file))\n",
    "                existing_expressions.update(small_df['Expression'].dropna().unique())\n",
    "    \n",
    "    # Remove entries that already exist in smaller len files\n",
    "    df = df[~df['Expression'].isin(existing_expressions)]\n",
    "    \n",
    "    # Remove duplicates and sort\n",
    "    df.drop_duplicates(subset=['Expression'], inplace=True)\n",
    "    df.sort_values(by=['Expression'], inplace=True)\n",
    "    \n",
    "    # Save back to CSV (cleaned version)\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "    \n",
    "    # Convert to Feather for faster access\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df.to_feather(feather_filename)\n",
    "    \n",
    "    print(f\"Cleaned and converted {csv_filename} → {feather_filename}\")\n",
    "\n",
    "for num_features in range(2, 8):\n",
    "    VARIABLES = [f\"X{i}\" for i in range(num_features)]\n",
    "    # Set parameters\n",
    "    if num_features in (2,3):\n",
    "        max_length = 11\n",
    "    else:\n",
    "        max_length = 9\n",
    "    save_interval = 100000\n",
    "    \n",
    "    # Generate expressions with proper batch processing and multiprocessing\n",
    "    generate_expressions(max_length, save_interval, num_features = num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551a960d-761c-4d29-aa97-af0c029514f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
